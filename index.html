<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-VGGNet" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/29/VGGNet/" class="article-date">
  <time class="dt-published" datetime="2024-10-29T11:43:13.962Z" itemprop="datePublished">2024-10-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h1><p><img src="https://i-blog.csdnimg.cn/blog_migrate/099343d77a56b17d2cdc6171eee5366a.png" alt="img"></p>
<p>模型架构图</p>
<h3 id="VGGNet-介绍"><a href="#VGGNet-介绍" class="headerlink" title="VGGNet 介绍"></a>VGGNet 介绍</h3><p>VGGNet 是由牛津大学的 Visual Geometry Group（VGG）在 2014 年提出的深度卷积神经网络（CNN）。VGGNet 在 ImageNet 大规模视觉识别挑战赛（ILSVRC）中取得了优异的成绩，其简洁的设计理念对后续的深度学习模型产生了深远的影响。</p>
<h4 id="主要特点"><a href="#主要特点" class="headerlink" title="主要特点"></a>主要特点</h4><ul>
<li><p>小卷积核：VGGNet 使用了较小的 3x3 卷积核，这有助于在网络中保持更多的空间信息，同时减少了参数数量。</p>
</li>
<li><p>深度：VGGNet 通过堆叠多个 3x3 卷积层来增加网络的深度，从而能够捕捉到更复杂的特征。</p>
</li>
<li><p>固定步长和填充：所有卷积层都使用相同的步长（stride&#x3D;1）和填充（padding&#x3D;1），这样可以确保每个卷积层的输出大小与输入相同，直到进行下采样操作。</p>
</li>
<li><p>最大池化：在每组卷积层之后，使用 2x2 最大池化层（max pooling）来减少特征图的尺寸，同时保留最重要的信息。</p>
</li>
<li><p>全连接层：网络的最后几层是全连接层，用于分类任务。</p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/29/VGGNet/" data-id="cm2udqpdi00007w66bgi7dr38" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-医学影像分割研究报告" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/29/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E5%88%86%E5%89%B2%E7%A0%94%E7%A9%B6%E6%8A%A5%E5%91%8A/" class="article-date">
  <time class="dt-published" datetime="2024-10-29T11:23:04.668Z" itemprop="datePublished">2024-10-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="医学影像分割研究报告"><a href="#医学影像分割研究报告" class="headerlink" title="医学影像分割研究报告"></a>医学影像分割研究报告</h1><h2 id="背景介绍与前置知识"><a href="#背景介绍与前置知识" class="headerlink" title="背景介绍与前置知识"></a>背景介绍与前置知识</h2><p><strong>图像分割</strong>定义为将整个图像划分为一组区域，在广泛的应用中发挥着重要作用。<strong>医学图像分割</strong>是该领域的一个关键例子，并为临床使用提供了许多好处。自动分割有助于加快数据处理时间，并通过提供特定任务的可视化和测量来指导临床医生。在几乎所有的临床应用中，可视化算法不仅提供了对人类组织异常区域的洞察，还指导从业者监测癌症进展。<strong>语义分割作</strong>为自动图像处理技术的预备步骤，可以通过建模来检测与手头任务更相关的特定区域（例如，心脏分割），从而进一步提高可视化质量。</p>
<p>图像分割任务可以分为两类：<strong>语义分割</strong>和<strong>实例分割</strong>（ <strong>semantic segmentation</strong> and <strong>instance segmentation</strong>）。语义分割是对图像中的所有像素进行像素级别的分类，而实例分割则需要在语义分割的基础上识别同一类别内的不同对象。常见的<strong>医疗成像模式</strong>包括X射线、正电子发射断层扫描(PET)、计算机断层扫描(CT)、磁共振成像(MRI)和超声波成像(US)。</p>
<h2 id="关键技术"><a href="#关键技术" class="headerlink" title="关键技术"></a>关键技术</h2><p>早期传统的医学图像分割方法主要集中在<strong>边缘检测</strong>、<strong>模板匹配技术</strong>、<strong>区域生长</strong>、<strong>图割法</strong>、<strong>活动轮廓线</strong>、<strong>机器学习</strong>以及其他数学方法上。近年来，<strong>深度学习</strong>在解决医疗领域许多边缘情况方面已经成熟并应用于多个领域。**卷积神经网络(CNNs)**已成功实现了图像特征表示的提取，从而消除了图像分割中手工制作特征的需要，其卓越的性能和准确性使它们成为该领域的首选。</p>
<blockquote>
<p>D. Ciresan, A. Giusti, L. Gambardella, and J. Schmidhuber,“Deep neural networks segment neuronal membranes in electron microscopy images,” Advances in neural information processing systems, vol. 25, 2012</p>
</blockquote>
<p>D. Ciresan,的论文中提出了使用深度神经网络建模语义分割的初步尝试。该方法将输入图像通过卷积编码器产生潜在表示。然后，在生成的特征图的顶部，包括完全连接的层以产生像素级的预测。这种架构的主要限制是使用完全连接的层，这耗尽了空间信息，从而降低了整体性能。Long等[6]提出了全卷积网络（Fully Convolutional Networks, fcn）来解决这种限制。FCN结构在编码器路径上应用由卷积层、激活层和池化层组成的几个卷积块来捕获语义表示，类似地，在解码路径中使用卷积层和上采样操作来提供像素级预测。</p>
<blockquote>
<p>J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for semantic segmentation,” arXiv preprint arXiv:1411.4038,2014.</p>
</blockquote>
<p><img src="C:\Users\17724\AppData\Roaming\Typora\typora-user-images\image-20241014144133806.png" alt="image-20241014144133806"></p>
<p>图1 Fully Convolutional Networks结构图</p>
<p>Ronneberger等人受到fcn架构和编码器-解码器模型的启发，开发了U-Net生物医学图像分割模型。</p>
<blockquote>
<p>O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” in International Conference on Medical image computing and computer-assisted intervention. Springer, 2015, pp. 234–241.</p>
</blockquote>
<p><img src="C:\Users\17724\AppData\Roaming\Typora\typora-user-images\image-20241014144511211.png" alt="image-20241014144511211"></p>
<p>图2 U-net结构图</p>
<p>U-Net网络由两部分组成。第一部分是收缩路径，它使用包含若干卷积块的下采样模块来提取语义和上下文特征。第二部分是扩展路径，它应用一系列配备上采样操作的卷积块，逐步增加特征图的空间分辨率，通常每次增加两倍，同时减少特征维度，以产生像素级别的分类得分。U-Net中最重要和显著的部分是跳跃连接，它们将收缩路径中每个阶段的输出复制到扩展路径中相应阶段。 这样的网络可以从很少的图像中进行端到端训练</p>
<h2 id="代码复现"><a href="#代码复现" class="headerlink" title="代码复现"></a>代码复现</h2><h3 id="unet网络结构"><a href="#unet网络结构" class="headerlink" title="unet网络结构"></a>unet网络结构</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#unet结构定义</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DoubleConv</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;(convolution =&gt; [BN] =&gt; ReLU) * 2&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, mid_channels=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> mid_channels:</span><br><span class="line">            mid_channels = out_channels</span><br><span class="line">        <span class="variable language_">self</span>.double_conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, mid_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(mid_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(mid_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.double_conv(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Down</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Downscaling with maxpool then double conv&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.maxpool_conv = nn.Sequential(</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            DoubleConv(in_channels, out_channels)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.maxpool_conv(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Up</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Upscaling then double conv&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, bilinear=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> bilinear:</span><br><span class="line">            <span class="variable language_">self</span>.up = nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">&#x27;bilinear&#x27;</span>, align_corners=<span class="literal">True</span>)</span><br><span class="line">            <span class="variable language_">self</span>.conv = DoubleConv(in_channels, out_channels, in_channels // <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.up = nn.ConvTranspose2d(in_channels, in_channels // <span class="number">2</span>, kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">            <span class="variable language_">self</span>.conv = DoubleConv(in_channels, out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x1, x2</span>):</span><br><span class="line">        x1 = <span class="variable language_">self</span>.up(x1)</span><br><span class="line">        <span class="comment"># input is CHW</span></span><br><span class="line">        diffY = x2.size()[<span class="number">2</span>] - x1.size()[<span class="number">2</span>]</span><br><span class="line">        diffX = x2.size()[<span class="number">3</span>] - x1.size()[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">        x1 = F.pad(x1, [diffX // <span class="number">2</span>, diffX - diffX // <span class="number">2</span>,</span><br><span class="line">                        diffY // <span class="number">2</span>, diffY - diffY // <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        x = torch.cat([x2, x1], dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.conv(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">OutConv</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(OutConv, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.conv(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment"># unet定义</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">UNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_channels, n_classes, bilinear=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="comment"># 定义</span></span><br><span class="line">        <span class="built_in">super</span>(UNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.n_channels = n_channels</span><br><span class="line">        <span class="variable language_">self</span>.n_classes = n_classes</span><br><span class="line">        <span class="variable language_">self</span>.bilinear = bilinear</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.inc = (DoubleConv(n_channels, <span class="number">64</span>))</span><br><span class="line">        <span class="variable language_">self</span>.down1 = (Down(<span class="number">64</span>, <span class="number">128</span>))</span><br><span class="line">        <span class="variable language_">self</span>.down2 = (Down(<span class="number">128</span>, <span class="number">256</span>))</span><br><span class="line">        <span class="variable language_">self</span>.down3 = (Down(<span class="number">256</span>, <span class="number">512</span>))</span><br><span class="line">        factor = <span class="number">2</span> <span class="keyword">if</span> bilinear <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        <span class="variable language_">self</span>.down4 = (Down(<span class="number">512</span>, <span class="number">1024</span> // factor))</span><br><span class="line">        <span class="variable language_">self</span>.up1 = (Up(<span class="number">1024</span>, <span class="number">512</span> // factor, bilinear))</span><br><span class="line">        <span class="variable language_">self</span>.up2 = (Up(<span class="number">512</span>, <span class="number">256</span> // factor, bilinear))</span><br><span class="line">        <span class="variable language_">self</span>.up3 = (Up(<span class="number">256</span>, <span class="number">128</span> // factor, bilinear))</span><br><span class="line">        <span class="variable language_">self</span>.up4 = (Up(<span class="number">128</span>, <span class="number">64</span>, bilinear))</span><br><span class="line">        <span class="variable language_">self</span>.outc = (OutConv(<span class="number">64</span>, n_classes))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 网络架构</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x1 = <span class="variable language_">self</span>.inc(x)</span><br><span class="line">        x2 = <span class="variable language_">self</span>.down1(x1)</span><br><span class="line">        x3 = <span class="variable language_">self</span>.down2(x2)</span><br><span class="line">        x4 = <span class="variable language_">self</span>.down3(x3)</span><br><span class="line">        x5 = <span class="variable language_">self</span>.down4(x4)</span><br><span class="line">        x = <span class="variable language_">self</span>.up1(x5, x4)</span><br><span class="line">        x = <span class="variable language_">self</span>.up2(x, x3)</span><br><span class="line">        x = <span class="variable language_">self</span>.up3(x, x2)</span><br><span class="line">        x = <span class="variable language_">self</span>.up4(x, x1)</span><br><span class="line">        logits = <span class="variable language_">self</span>.outc(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">use_checkpointing</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.inc = torch.utils.checkpoint(<span class="variable language_">self</span>.inc)</span><br><span class="line">        <span class="variable language_">self</span>.down1 = torch.utils.checkpoint(<span class="variable language_">self</span>.down1)</span><br><span class="line">        <span class="variable language_">self</span>.down2 = torch.utils.checkpoint(<span class="variable language_">self</span>.down2)</span><br><span class="line">        <span class="variable language_">self</span>.down3 = torch.utils.checkpoint(<span class="variable language_">self</span>.down3)</span><br><span class="line">        <span class="variable language_">self</span>.down4 = torch.utils.checkpoint(<span class="variable language_">self</span>.down4)</span><br><span class="line">        <span class="variable language_">self</span>.up1 = torch.utils.checkpoint(<span class="variable language_">self</span>.up1)</span><br><span class="line">        <span class="variable language_">self</span>.up2 = torch.utils.checkpoint(<span class="variable language_">self</span>.up2)</span><br><span class="line">        <span class="variable language_">self</span>.up3 = torch.utils.checkpoint(<span class="variable language_">self</span>.up3)</span><br><span class="line">        <span class="variable language_">self</span>.up4 = torch.utils.checkpoint(<span class="variable language_">self</span>.up4)</span><br><span class="line">        <span class="variable language_">self</span>.outc = torch.utils.checkpoint(<span class="variable language_">self</span>.outc)</span><br></pre></td></tr></table></figure>

<h3 id="unet训练代码"><a href="#unet训练代码" class="headerlink" title="unet训练代码"></a>unet训练代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params"></span></span><br><span class="line"><span class="params">        model,</span></span><br><span class="line"><span class="params">        device,</span></span><br><span class="line"><span class="params">        epochs: <span class="built_in">int</span> = <span class="number">20</span>,</span></span><br><span class="line"><span class="params">        batch_size: <span class="built_in">int</span> = <span class="number">1</span>,</span></span><br><span class="line"><span class="params">        learning_rate: <span class="built_in">float</span> = <span class="number">1e-5</span>,</span></span><br><span class="line"><span class="params">        val_percent: <span class="built_in">float</span> = <span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">        save_checkpoint: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        img_scale: <span class="built_in">float</span> = <span class="number">0.5</span>,</span></span><br><span class="line"><span class="params">        amp: <span class="built_in">bool</span> = <span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        weight_decay: <span class="built_in">float</span> = <span class="number">1e-8</span>,</span></span><br><span class="line"><span class="params">        momentum: <span class="built_in">float</span> = <span class="number">0.999</span>,</span></span><br><span class="line"><span class="params">        gradient_clipping: <span class="built_in">float</span> = <span class="number">1.0</span>,</span></span><br><span class="line"><span class="params"></span>):  </span><br><span class="line">    tran_list = [transforms.Resize((<span class="number">1024</span>,<span class="number">768</span>)), transforms.ToTensor(),]</span><br><span class="line">    transform_train = transforms.Compose(tran_list)</span><br><span class="line">    <span class="comment"># 1. 创建数据集</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        dataset = CarvanaDataset(transform_train,dir_img, dir_mask, img_scale)</span><br><span class="line">    <span class="keyword">except</span> (AssertionError, RuntimeError, IndexError):</span><br><span class="line">        dataset = BasicDataset(transform_train,dir_img, dir_mask, img_scale)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 分割训练/验证集</span></span><br><span class="line">    n_val = <span class="built_in">int</span>(<span class="built_in">len</span>(dataset) * val_percent)</span><br><span class="line">    n_train = <span class="built_in">len</span>(dataset) - n_val</span><br><span class="line">    train_set, val_set = random_split(dataset, [n_train, n_val], generator=torch.Generator().manual_seed(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 创建 data loaders</span></span><br><span class="line">    </span><br><span class="line">    loader_args = <span class="built_in">dict</span>(batch_size=batch_size, num_workers=os.cpu_count(), pin_memory=<span class="literal">True</span>)</span><br><span class="line">    train_loader = DataLoader(train_set, shuffle=<span class="literal">True</span>, **loader_args)</span><br><span class="line">    val_loader = DataLoader(val_set, shuffle=<span class="literal">False</span>, drop_last=<span class="literal">True</span>, **loader_args)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化记录</span></span><br><span class="line">    <span class="comment"># experiment = wandb.init(project=&#x27;U-Net&#x27;, resume=&#x27;allow&#x27;, anonymous=&#x27;must&#x27;)</span></span><br><span class="line">    <span class="comment"># experiment.config.update(</span></span><br><span class="line">    <span class="comment">#     dict(epochs=epochs, batch_size=batch_size, learning_rate=learning_rate,</span></span><br><span class="line">    <span class="comment">#          val_percent=val_percent, save_checkpoint=save_checkpoint, img_scale=img_scale, amp=amp)</span></span><br><span class="line">    <span class="comment"># )</span></span><br><span class="line"></span><br><span class="line">    logging.info(<span class="string">f&#x27;&#x27;&#x27;Starting training:</span></span><br><span class="line"><span class="string">        Epochs:          <span class="subst">&#123;epochs&#125;</span></span></span><br><span class="line"><span class="string">        Batch size:      <span class="subst">&#123;batch_size&#125;</span></span></span><br><span class="line"><span class="string">        Learning rate:   <span class="subst">&#123;learning_rate&#125;</span></span></span><br><span class="line"><span class="string">        Training size:   <span class="subst">&#123;n_train&#125;</span></span></span><br><span class="line"><span class="string">        Validation size: <span class="subst">&#123;n_val&#125;</span></span></span><br><span class="line"><span class="string">        Checkpoints:     <span class="subst">&#123;save_checkpoint&#125;</span></span></span><br><span class="line"><span class="string">        Device:          <span class="subst">&#123;device.<span class="built_in">type</span>&#125;</span></span></span><br><span class="line"><span class="string">        Images scaling:  <span class="subst">&#123;img_scale&#125;</span></span></span><br><span class="line"><span class="string">        Mixed Precision: <span class="subst">&#123;amp&#125;</span></span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. 优化器，损失 学习率</span></span><br><span class="line">    <span class="comment">#  优化器更新</span></span><br><span class="line">    <span class="comment"># optimizer = optim.RMSprop(model.parameters(),</span></span><br><span class="line">    <span class="comment">#                           lr=learning_rate, weight_decay=weight_decay, momentum=momentum, foreach=True)</span></span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)</span><br><span class="line">    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, <span class="string">&#x27;max&#x27;</span>, patience=<span class="number">5</span>)  <span class="comment"># goal: maximize Dice score</span></span><br><span class="line">    grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)</span><br><span class="line">    criterion = nn.CrossEntropyLoss() <span class="keyword">if</span> model.n_classes &gt; <span class="number">1</span> <span class="keyword">else</span> nn.BCEWithLogitsLoss()</span><br><span class="line">    global_step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. 开始训练</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">        model.train()</span><br><span class="line">        epoch_loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">with</span> tqdm(total=n_train, desc=<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch&#125;</span>/<span class="subst">&#123;epochs&#125;</span>&#x27;</span>, unit=<span class="string">&#x27;img&#x27;</span>) <span class="keyword">as</span> pbar:</span><br><span class="line">            <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">                images, true_masks = batch[<span class="string">&#x27;image&#x27;</span>], batch[<span class="string">&#x27;mask&#x27;</span>]</span><br><span class="line"></span><br><span class="line">                <span class="keyword">assert</span> images.shape[<span class="number">1</span>] == model.n_channels, \</span><br><span class="line">                    <span class="string">f&#x27;Network has been defined with <span class="subst">&#123;model.n_channels&#125;</span> input channels, &#x27;</span> \</span><br><span class="line">                    <span class="string">f&#x27;but loaded images have <span class="subst">&#123;images.shape[<span class="number">1</span>]&#125;</span> channels. Please check that &#x27;</span> \</span><br><span class="line">                    <span class="string">&#x27;the images are loaded correctly.&#x27;</span></span><br><span class="line"></span><br><span class="line">                images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)</span><br><span class="line">                true_masks = true_masks.to(device=device, dtype=torch.long)</span><br><span class="line">                <span class="comment">#  计算损失</span></span><br><span class="line">                <span class="keyword">with</span> torch.autocast(device.<span class="built_in">type</span> <span class="keyword">if</span> device.<span class="built_in">type</span> != <span class="string">&#x27;mps&#x27;</span> <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>, enabled=amp):</span><br><span class="line">                    masks_pred = model(images)</span><br><span class="line">                    <span class="keyword">if</span> model.n_classes == <span class="number">1</span>:</span><br><span class="line">                        loss = criterion(masks_pred.squeeze(<span class="number">1</span>), true_masks.<span class="built_in">float</span>())</span><br><span class="line">                        loss += dice_loss(F.sigmoid(masks_pred.squeeze(<span class="number">1</span>)), true_masks.<span class="built_in">float</span>(), multiclass=<span class="literal">False</span>)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        loss = criterion(masks_pred, true_masks)</span><br><span class="line">                        loss += dice_loss(</span><br><span class="line">                            F.softmax(masks_pred, dim=<span class="number">1</span>).<span class="built_in">float</span>(),</span><br><span class="line">                            F.one_hot(true_masks, model.n_classes).permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>).<span class="built_in">float</span>(),</span><br><span class="line">                            multiclass=<span class="literal">True</span></span><br><span class="line">                        )</span><br><span class="line">                <span class="comment"># 梯度更新</span></span><br><span class="line">                optimizer.zero_grad(set_to_none=<span class="literal">True</span>)</span><br><span class="line">                grad_scaler.scale(loss).backward()</span><br><span class="line">                grad_scaler.unscale_(optimizer)</span><br><span class="line">                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)</span><br><span class="line">                grad_scaler.step(optimizer)</span><br><span class="line">                grad_scaler.update()</span><br><span class="line"></span><br><span class="line">                pbar.update(images.shape[<span class="number">0</span>])</span><br><span class="line">                <span class="comment"># 输出</span></span><br><span class="line">                global_step += <span class="number">1</span></span><br><span class="line">                epoch_loss += loss.item()</span><br><span class="line">                <span class="comment"># experiment.log(&#123;</span></span><br><span class="line">                <span class="comment">#     &#x27;train loss&#x27;: loss.item(),</span></span><br><span class="line">                <span class="comment">#     &#x27;step&#x27;: global_step,</span></span><br><span class="line">                <span class="comment">#     &#x27;epoch&#x27;: epoch</span></span><br><span class="line">                <span class="comment"># &#125;)</span></span><br><span class="line">                pbar.set_postfix(**&#123;<span class="string">&#x27;loss (batch)&#x27;</span>: loss.item()&#125;)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 验证</span></span><br><span class="line">                division_step = (n_train // (<span class="number">5</span> * batch_size))</span><br><span class="line">                <span class="keyword">if</span> division_step &gt; <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">if</span> global_step % division_step == <span class="number">0</span>:</span><br><span class="line">                        histograms = &#123;&#125;</span><br><span class="line">                        <span class="keyword">for</span> tag, value <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">                            tag = tag.replace(<span class="string">&#x27;/&#x27;</span>, <span class="string">&#x27;.&#x27;</span>)</span><br><span class="line">                            <span class="comment"># if not (torch.isinf(value) | torch.isnan(value)).any():</span></span><br><span class="line">                            <span class="comment">#     histograms[&#x27;Weights/&#x27; + tag] = wandb.Histogram(value.data.cpu())</span></span><br><span class="line">                            <span class="comment"># if not (torch.isinf(value.grad) | torch.isnan(value.grad)).any():</span></span><br><span class="line">                            <span class="comment">#     histograms[&#x27;Gradients/&#x27; + tag] = wandb.Histogram(value.grad.data.cpu())</span></span><br><span class="line"></span><br><span class="line">                        val_score = evaluate(model, val_loader, device, amp)</span><br><span class="line">                        scheduler.step(val_score)</span><br><span class="line"></span><br><span class="line">                        logging.info(<span class="string">&#x27;Validation Dice score: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(val_score))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> save_checkpoint:</span><br><span class="line">            <span class="comment"># 保存记录点</span></span><br><span class="line">            Path(dir_checkpoint).mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">            state_dict = model.state_dict()</span><br><span class="line">            state_dict[<span class="string">&#x27;mask_values&#x27;</span>] = dataset.mask_values</span><br><span class="line">            torch.save(state_dict, <span class="built_in">str</span>(dir_checkpoint / <span class="string">&#x27;checkpoint_epoch&#123;&#125;.pth&#x27;</span>.<span class="built_in">format</span>(epoch)))</span><br><span class="line">            logging.info(<span class="string">f&#x27;Checkpoint <span class="subst">&#123;epoch&#125;</span> saved!&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_args</span>():</span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">&#x27;Train the UNet on images and target masks&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--epochs&#x27;</span>, <span class="string">&#x27;-e&#x27;</span>, metavar=<span class="string">&#x27;E&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">15</span>, <span class="built_in">help</span>=<span class="string">&#x27;Number of epochs&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--batch-size&#x27;</span>, <span class="string">&#x27;-b&#x27;</span>, dest=<span class="string">&#x27;batch_size&#x27;</span>, metavar=<span class="string">&#x27;B&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">8</span>, <span class="built_in">help</span>=<span class="string">&#x27;Batch size&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--learning-rate&#x27;</span>, <span class="string">&#x27;-l&#x27;</span>, metavar=<span class="string">&#x27;LR&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">1e-5</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;Learning rate&#x27;</span>, dest=<span class="string">&#x27;lr&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--load&#x27;</span>, <span class="string">&#x27;-f&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="literal">False</span>, <span class="built_in">help</span>=<span class="string">&#x27;Load model from a .pth file&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--scale&#x27;</span>, <span class="string">&#x27;-s&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.5</span>, <span class="built_in">help</span>=<span class="string">&#x27;Downscaling factor of the images&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--validation&#x27;</span>, <span class="string">&#x27;-v&#x27;</span>, dest=<span class="string">&#x27;val&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">20.0</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;Percent of the data that is used as validation (0-100)&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--amp&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>, <span class="built_in">help</span>=<span class="string">&#x27;Use mixed precision&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--bilinear&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>, <span class="built_in">help</span>=<span class="string">&#x27;Use bilinear upsampling&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--classes&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">2</span>, <span class="built_in">help</span>=<span class="string">&#x27;Number of classes&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parser.parse_args()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment">#  获取参数</span></span><br><span class="line">    args = get_args()</span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 记录</span></span><br><span class="line">    logging.basicConfig(level=logging.INFO, <span class="built_in">format</span>=<span class="string">&#x27;%(levelname)s: %(message)s&#x27;</span>)</span><br><span class="line">    logging.info(<span class="string">f&#x27;Using device <span class="subst">&#123;device&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># n_channels=3 for RGB images</span></span><br><span class="line">    model = UNet(n_channels=<span class="number">3</span>, n_classes=args.classes, bilinear=args.bilinear)</span><br><span class="line">    model = model.to(memory_format=torch.channels_last)</span><br><span class="line"></span><br><span class="line">    logging.info(<span class="string">f&#x27;Network:\n&#x27;</span></span><br><span class="line">                 <span class="string">f&#x27;\t<span class="subst">&#123;model.n_channels&#125;</span> input channels\n&#x27;</span></span><br><span class="line">                 <span class="string">f&#x27;\t<span class="subst">&#123;model.n_classes&#125;</span> output channels (classes)\n&#x27;</span></span><br><span class="line">                 <span class="string">f&#x27;\t<span class="subst">&#123;<span class="string">&quot;Bilinear&quot;</span> <span class="keyword">if</span> model.bilinear <span class="keyword">else</span> <span class="string">&quot;Transposed conv&quot;</span>&#125;</span> upscaling&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 继续训练</span></span><br><span class="line">    <span class="keyword">if</span> args.load:</span><br><span class="line">        state_dict = torch.load(args.load, map_location=device)</span><br><span class="line">        <span class="keyword">del</span> state_dict[<span class="string">&#x27;mask_values&#x27;</span>]</span><br><span class="line">        model.load_state_dict(state_dict)</span><br><span class="line">        logging.info(<span class="string">f&#x27;Model loaded from <span class="subst">&#123;args.load&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    model.to(device=device)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        train_model(</span><br><span class="line">            model=model,</span><br><span class="line">            epochs=args.epochs,</span><br><span class="line">            batch_size=args.batch_size,</span><br><span class="line">            learning_rate=args.lr,</span><br><span class="line">            device=device,</span><br><span class="line">            img_scale=args.scale,</span><br><span class="line">            val_percent=args.val / <span class="number">100</span>,</span><br><span class="line">            amp=args.amp</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">except</span> torch.cuda.OutOfMemoryError:</span><br><span class="line">        <span class="comment"># 爆内存继续训练</span></span><br><span class="line">        logging.error(<span class="string">&#x27;OutOfMemoryError! &#x27;</span>)</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line">        model.use_checkpointing()</span><br><span class="line">        train_model(</span><br><span class="line">            model=model,</span><br><span class="line">            epochs=args.epochs,</span><br><span class="line">            batch_size=args.batch_size,</span><br><span class="line">            learning_rate=args.lr,</span><br><span class="line">            device=device,</span><br><span class="line">            img_scale=args.scale,</span><br><span class="line">            val_percent=args.val / <span class="number">100</span>,</span><br><span class="line">            amp=args.amp</span><br><span class="line">        )</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><p>我们使用了ISBI2016_Part数据集，该数据集包含了一系列医学图像及其对应的分割掩码。这些图像主要用于细胞核的分割任务。实验运行在配备NVIDIA GeForce RTX 3090显卡的机器上，具有24GB的显存，确保了模型训练的高效性。基于PyTorch框架进行，使用Python编程语言。所有实验代码均在Python 3.11.9环境下运行，PyTorch版本为2.4。采用经典的UNet模型，输入通道数为3（适用于RGB图像），输出类别数为2（二分类任务）。模型采用了双线性上采样方式（bilinear upsampling）。Batch Size设置为1，初始学习率为1e-5，使用Adam优化器进行参数更新，结合了二元交叉熵损失（BCEWithLogitsLoss）和Dice损失，以提高模型在分割任务中的性能。总共训练了20个epoch，每个epoch遍历一次完整的训练数据集。从整个数据集中随机抽取10%作为验证集，用于监控模型的泛化性能。</p>
<p>在前3个epoch中，训练损失和验证损失迅速下降，表明模型在快速学习数据中的特征。从第4个epoch开始，训练损失和验证损失的下降速度显著减缓，损失值趋于稳定。这表明模型已经找到了一个相对较好的参数配置，进一步的训练可能只会带来微小的改进。在第4个epoch之后，模型在验证集上的Dice系数达到0.81，表明模型具有良好的泛化能力和预测准确性。通过观察验证集上的Dice系数，我们可以发现模型在第4个epoch之后的表现已经非常稳定，没有明显的过拟合或欠拟合现象。在测试集上的准确率为0.83</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/29/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E5%88%86%E5%89%B2%E7%A0%94%E7%A9%B6%E6%8A%A5%E5%91%8A/" data-id="cm2blrj5b00015k66fmxhet3j" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/16/hello-world/" class="article-date">
  <time class="dt-published" datetime="2024-10-16T07:49:29.393Z" itemprop="datePublished">2024-10-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="深度学习之旅"><a href="#深度学习之旅" class="headerlink" title="深度学习之旅"></a>深度学习之旅</h1><p>2022201719 于丰华</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/16/hello-world/" data-id="cm2blrj5800005k660ykfdmet" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">October 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/10/29/VGGNet/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/10/29/%E5%8C%BB%E5%AD%A6%E5%BD%B1%E5%83%8F%E5%88%86%E5%89%B2%E7%A0%94%E7%A9%B6%E6%8A%A5%E5%91%8A/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/10/16/hello-world/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>